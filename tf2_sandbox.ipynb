{"cells":[{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["from __future__ import absolute_import, division, print_function, unicode_literals"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# Import system helpers\n","import os\n","import pathlib\n","from os import listdir\n","from os.path import isfile, join\n","\n","# Import TensorFlow and tf.keras\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import datasets, layers, models\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Import helper libraries\n","import numpy as np\n","import pandas as pd\n","import scipy.io\n","import matplotlib.pyplot as plt\n","import IPython.display as display\n","from PIL import Image\n","\n","# Import pipeline\n","import format_data\n","import write_tf2record"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["car_makes, cars_train_labels, cars_train_annotations, cars_test_annotations = format_data.devkit()\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# def _bytes_feature(value):\n","#   \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n","#   if isinstance(value, type(tf.constant(0))):\n","#     value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n","#   return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n","\n","# def _float_feature(value):\n","#   \"\"\"Returns a float_list from a float / double.\"\"\"\n","#   return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n","\n","# def _int64_feature(value):\n","#   \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n","#   return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# # This is an example, just using the cat image.\n","# path = '/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy/'\n","# img = '00001.jpg'\n","# image_string = open(path+img, 'rb').read()\n","# label = cars_train_labels[img]\n","\n","# # Create a dictionary with features that may be relevant.\n","# def image_example(image_string, label):\n","#   image_shape = tf.image.decode_jpeg(image_string).shape\n","\n","#   feature = {\n","#       'height': _int64_feature(image_shape[0]),\n","#       'width': _int64_feature(image_shape[1]),\n","#       'depth': _int64_feature(image_shape[2]),\n","#       'label': _int64_feature(label),\n","#       'image_raw': _bytes_feature(image_string),\n","#   }\n","\n","#   return tf.train.Example(features=tf.train.Features(feature=feature))\n","\n","# #for line in str(image_example(image_string, label)).split('\\n')[:15]:\n","# #  print(line)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# record_file = 'images.tfrecords'\n","# with tf.io.TFRecordWriter(record_file) as writer:\n","#   for filename, label in cars_train_labels.items():\n","#     image_string = open(filfilename, 'rb').read()\n","#     tf_example = image_example(image_string, label)\n","#     writer.write(tf_example.SerializeToString())"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# !du -sh {record_file}"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# raw_image_dataset = tf.data.TFRecordDataset('images.tfrecords')\n","\n","# # Create a dictionary describing the features.\n","# image_feature_description = {\n","#     'height': tf.io.FixedLenFeature([], tf.int64),\n","#     'width': tf.io.FixedLenFeature([], tf.int64),\n","#     'depth': tf.io.FixedLenFeature([], tf.int64),\n","#     'label': tf.io.FixedLenFeature([], tf.int64),\n","#     'image_raw': tf.io.FixedLenFeature([], tf.string),\n","# }\n","\n","# def _parse_image_function(example_proto):\n","#   # Parse the input tf.Example proto using the dictionary above.\n","#   return tf.io.parse_single_example(example_proto, image_feature_description)\n","\n","# parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n","# parsed_image_dataset"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["parsed_image_dataset = write_tf2record.read_record('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy/')"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["labels = [image_features['label'].numpy() for image_features in parsed_image_dataset]\n","images = np.array([image_features['image_raw'].numpy() for image_features in parsed_image_dataset])"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":"images = np.array([tf.image.resize(tf.image.decode_jpeg(img, channels=3), [224, 224]) for img in images])"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"# image_batch, label_batch = format_data.get_batch('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/*')"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"# x = label_batch[0]\n# car_makes.index[x]"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"# YOU SHOULD ONLY CALL THIS ONCE PER DIRECTORY\n# format_data.label_crop('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/')"},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":"# def get_label(file_path):\n#     # convert the path to a list of path components\n#     parts = tf.strings.split(tf.strings.split(file_path, '/')[-1], '_')[0]\n#     # The last is the image name\n#     return parts==np.array([str(label) for label in cars_train_labels.values()])\n\n# label = get_label('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/196_07537.jpg')"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"# # Crop images based on bounding box\n\n# #copy_path = '/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/'\n# def crop(copy_path): \n#     for filename in os.listdir(copy_path):\n#         img = Image.open(copy_path + filename)\n#         bb_x1 = int(cars_train_annotations[cars_train_annotations['img_name'] == filename]['bb_x1'])\n#         bb_x2 = int(cars_train_annotations[cars_train_annotations['img_name'] == filename]['bb_x2'])\n#         bb_y1 = int(cars_train_annotations[cars_train_annotations['img_name'] == filename]['bb_y1'])\n#         bb_y2 = int(cars_train_annotations[cars_train_annotations['img_name'] == filename]['bb_y2'])\n#         img2 = img.crop((bb_x1, bb_x2, bb_y1, bb_y2))\n#         img2.save(copy_path + filename)"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"# #crop(copy_path='/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/')\n# img = Image.open('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/00004.jpg')\n# width, height = img.size\n# bb_x1 = int(cars_train_annotations[cars_train_annotations['img_name'] == '00004.jpg']['bb_x1'])\n# bb_x2 = int(cars_train_annotations[cars_train_annotations['img_name'] == '00004.jpg']['bb_x2'])\n# bb_y1 = int(cars_train_annotations[cars_train_annotations['img_name'] == '00004.jpg']['bb_y1'])\n# bb_y2 = int(cars_train_annotations[cars_train_annotations['img_name'] == '00004.jpg']['bb_y2'])\n# img2 = img.crop((bb_x1, bb_x2, bb_y1, bb_y2))\n\n# #print(width, height, bb_x1, bb_x2, bb_y1, bb_y2)\n# img2.save(copy_path + filename)\n# #img2 = Image.open('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/a_00004.jpg')\n# img2.show()"},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":"# # Convert .mat to Python dict with Scipy.io\n\n# sc_devkit = '/Users/katerina/Workspace/visual_census/data/devkit'\n# cars_meta = scipy.io.loadmat(os.path.join(sc_devkit, 'cars_meta.mat'))['class_names']\n# cars_train_annos = scipy.io.loadmat(os.path.join(sc_devkit, 'cars_train_annos.mat'))['annotations']"},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":"# # Extract Car Metadata from dictionary to an array\n\n# car_makes = []\n# for idx, vehicle in enumerate(cars_meta[0]):\n#     car_makes.append([vehicle[0],vehicle[0].split(' ')[0], vehicle[0].split(' ')[-1]])\n# car_makes = pd.DataFrame(car_makes, columns=['full_label', 'mnfr', 'year'])\n# car_make_nums = np.array(car_makes.index)"},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":"# # Extract Car Labels from dictionary to a list\n# # List used because both int (label) and str (img id) required\n\n# cars_train_labels = {}\n# cars_train_annotations = []\n# for idx, anno in enumerate(cars_train_annos[0]):\n#     cars_train_labels[anno[5][0]] = anno[4][0][0]\n#     cars_train_annotations.append([anno[0][0][0], anno[1][0][0], anno[2][0][0], anno[3][0][0], anno[4][0][0], anno[5][0]])\n# cars_train_annotations = pd.DataFrame(cars_train_annotations, columns=['bb0', 'bb1', 'bb2', 'bb3', 'label', 'img_name'])"},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":"# sc_cars_train = '/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy'\n# sc_cars_test = '/Users/katerina/Workspace/visual_census/data/training_data/cars_test'\n# train_data_dir = pathlib.Path(sc_cars_train)\n# test_data_dir = pathlib.Path(sc_cars_test)"},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":"# train_image_count = len(list(train_data_dir.glob('*.jpg')))\n# test_image_count = len(list(test_data_dir.glob('*.jpg')))"},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":"# train_image_names = np.array([item.name for item in train_data_dir.glob('*')])\n# test_image_names = np.array([item.name for item in test_data_dir.glob('*')])"},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":"# list_ds = tf.data.Dataset.list_files(str('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy/*'))\n# list_ds_test = tf.data.Dataset.list_files(str('/Users/katerina/Workspace/visual_census/data/training_data/cars_test/*'))"},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":"# BATCH_SIZE = 32\n# IMG_HEIGHT = 224\n# IMG_WIDTH = 224\n# STEPS_PER_EPOCH = np.ceil(train_image_count/BATCH_SIZE)"},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":"# def get_label(file_path):\n#   # convert the path to a list of path components\n#   parts = tf.strings.split(tf.strings.split(file_path, '_')[-1], '.')[0]\n#   # The last is the image name\n#   return parts==np.array([str(idx) for idx in car_makes.index])"},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":"# def decode_img(img):\n#   # convert the compressed string to a 3D uint8 tensor\n#   img = tf.image.decode_jpeg(img, channels=3)\n#   # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n#   img = tf.image.convert_image_dtype(img, tf.float32)\n#   # resize the image to the desired size.\n#   return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])"},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":"# def process_path(file_path):\n#   label = get_label(file_path)\n#   # load the raw data from the file as a string\n#   img = tf.io.read_file(file_path)\n#   img = decode_img(img)\n#   return img, label"},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":"# AUTOTUNE = tf.data.experimental.AUTOTUNE\n# labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n# labeled_ds_test = list_ds_test.map(process_path, num_parallel_calls=AUTOTUNE)"},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":"# def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n  \n#   if cache:\n#     if isinstance(cache, str):\n#       ds = ds.cache(cache)\n#     else:\n#       ds = ds.cache()\n\n#   ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n\n#   # Repeat forever\n#   ds = ds.repeat()\n\n#   ds = ds.batch(BATCH_SIZE)\n\n#   # `prefetch` lets the dataset fetch batches in the background while the model\n#   # is training.\n#   ds = ds.prefetch(buffer_size=AUTOTUNE)\n\n#   return ds"},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":"# train_ds = prepare_for_training(labeled_ds)\n# image_batch, label_batch = next(iter(train_ds))"},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":"# test_ds = labeled_ds_test.prefetch(buffer_size=AUTOTUNE)\n# image_batch_test, label_batch_test = next(iter(test_ds))"},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":"#np.array([x[label][0] for label in label_batch]).reshape(-1,1)"},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":"# def show_batch(image_batch, label_batch):\n#   plt.figure(figsize=(10,10))\n#   for n in range(25):\n#       ax = plt.subplot(5,5,n+1)\n#       plt.imshow(image_batch[n])\n#       plt.title(car_make_nums[label_batch[n]==1][0])\n#       plt.axis('off')\n\n# show_batch(image_batch.numpy(), label_batch.numpy())"},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":"# plt.figure(figsize=(10,10))\n# for i in range(25):\n#     plt.subplot(5,5,i+1)\n#     plt.xticks([])\n#     plt.yticks([])\n#     plt.grid(False)\n#     plt.imshow(image_batch[i], cmap=plt.cm.binary)\n#     plt.xlabel(np.array(car_makes['mnfr'])[label_batch[i]])\n# plt.show()"},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(196, activation='softmax'))"},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_6 (Conv2D)            (None, 222, 222, 32)      896       \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 111, 111, 32)      0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 109, 109, 64)      18496     \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 54, 54, 64)        0         \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 52, 52, 64)        36928     \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 173056)            0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                11075648  \n_________________________________________________________________\ndense_5 (Dense)              (None, 196)               12740     \n=================================================================\nTotal params: 11,144,708\nTrainable params: 11,144,708\nNon-trainable params: 0\n_________________________________________________________________\n"}],"source":"model.summary()"},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Failed to convert a NumPy array to a Tensor (Unsupported object type Int64Index).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-993cbf9891ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcar_makes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    607\u001b[0m   \u001b[0;31m# As a fallback for the data type that does not work with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[0;31m# _standardize_user_data, use the _prepare_model_with_inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     dataset = dataset_ops.DatasetV2.zip((\n\u001b[1;32m    320\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     ))\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    412\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   2333\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2335\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2336\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2337\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 111\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m    112\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1182\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1240\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    225\u001b[0m   \"\"\"\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type Int64Index)."]}],"source":"cars_numerical = np.array(car_makes.index)\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(image_batch, np.array([car_makes.index[label] for label in label_batch]), epochs=10)"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":"array([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False,  True, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False])"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"x == np.array([str(label) for label in car_makes.index])"},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"data":{"text/plain":"str"},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":"model.summary()"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}