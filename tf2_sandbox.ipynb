{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from __future__ import absolute_import, division, print_function, unicode_literals"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Import system helpers\n","import os\n","import pathlib\n","from os import listdir\n","from os.path import isfile, join\n","\n","# Import TensorFlow and tf.keras\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import datasets, layers, models\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Import helper libraries\n","import numpy as np\n","import pandas as pd\n","import scipy.io\n","import matplotlib.pyplot as plt\n","import IPython.display as display\n","from PIL import Image\n","\n","# Import pipeline\n","import format_data\n","import write_tf2record"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["car_makes, cars_train_labels, cars_train_annotations, cars_test_annotations = format_data.devkit()\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"# def _bytes_feature(value):\n#   \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n#   if isinstance(value, type(tf.constant(0))):\n#     value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n#   return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n# def _float_feature(value):\n#   \"\"\"Returns a float_list from a float / double.\"\"\"\n#   return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\n# def _int64_feature(value):\n#   \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n#   return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# # This is an example, just using the cat image.\n","# path = '/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy/'\n","# img = '00001.jpg'\n","# image_string = open(path+img, 'rb').read()\n","# label = cars_train_labels[img]\n","\n","# # Create a dictionary with features that may be relevant.\n","# def image_example(image_string, label):\n","#   image_shape = tf.image.decode_jpeg(image_string).shape\n","\n","#   feature = {\n","#       'height': _int64_feature(image_shape[0]),\n","#       'width': _int64_feature(image_shape[1]),\n","#       'depth': _int64_feature(image_shape[2]),\n","#       'label': _int64_feature(label),\n","#       'image_raw': _bytes_feature(image_string),\n","#   }\n","\n","#   return tf.train.Example(features=tf.train.Features(feature=feature))\n","\n","# #for line in str(image_example(image_string, label)).split('\\n')[:15]:\n","# #  print(line)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# record_file = 'images.tfrecords'\n","# with tf.io.TFRecordWriter(record_file) as writer:\n","#   for filename, label in cars_train_labels.items():\n","#     image_string = open(filfilename, 'rb').read()\n","#     tf_example = image_example(image_string, label)\n","#     writer.write(tf_example.SerializeToString())"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# !du -sh {record_file}"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# raw_image_dataset = tf.data.TFRecordDataset('images.tfrecords')\n","\n","# # Create a dictionary describing the features.\n","# image_feature_description = {\n","#     'height': tf.io.FixedLenFeature([], tf.int64),\n","#     'width': tf.io.FixedLenFeature([], tf.int64),\n","#     'depth': tf.io.FixedLenFeature([], tf.int64),\n","#     'label': tf.io.FixedLenFeature([], tf.int64),\n","#     'image_raw': tf.io.FixedLenFeature([], tf.string),\n","# }\n","\n","# def _parse_image_function(example_proto):\n","#   # Parse the input tf.Example proto using the dictionary above.\n","#   return tf.io.parse_single_example(example_proto, image_feature_description)\n","\n","# parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n","# parsed_image_dataset"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["parsed_image_dataset = write_tf2record.read_record('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy/')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"labels = [image_features['label'].numpy() for image_features in parsed_image_dataset]\nimages = np.array([image_features['image_raw'].numpy() for image_features in parsed_image_dataset])"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"10.9 ms ± 410 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"}],"source":"%%timeit\ntf.image.resize(tf.image.convert_image_dtype(tf.image.decode_jpeg(images[0], channels=3), tf.float32), [224, 224])\n#images = np.array([tf.image.resize(tf.image.decode_jpeg(img, channels=3), [224, 224]) for img in images])"},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":"images_decoded = [format_data.decode_img(img) for img in images[:1000]]"},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":"labels = np.array(labels)"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"# image_batch, label_batch = format_data.get_batch('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/*')"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"# x = label_batch[0]\n# car_makes.index[x]"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"# YOU SHOULD ONLY CALL THIS ONCE PER DIRECTORY\n# format_data.label_crop('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/')"},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":"# def get_label(file_path):\n#     # convert the path to a list of path components\n#     parts = tf.strings.split(tf.strings.split(file_path, '/')[-1], '_')[0]\n#     # The last is the image name\n#     return parts==np.array([str(label) for label in cars_train_labels.values()])\n\n# label = get_label('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/196_07537.jpg')"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"# # Crop images based on bounding box\n\n# #copy_path = '/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/'\n# def crop(copy_path): \n#     for filename in os.listdir(copy_path):\n#         img = Image.open(copy_path + filename)\n#         bb_x1 = int(cars_train_annotations[cars_train_annotations['img_name'] == filename]['bb_x1'])\n#         bb_x2 = int(cars_train_annotations[cars_train_annotations['img_name'] == filename]['bb_x2'])\n#         bb_y1 = int(cars_train_annotations[cars_train_annotations['img_name'] == filename]['bb_y1'])\n#         bb_y2 = int(cars_train_annotations[cars_train_annotations['img_name'] == filename]['bb_y2'])\n#         img2 = img.crop((bb_x1, bb_x2, bb_y1, bb_y2))\n#         img2.save(copy_path + filename)"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"# #crop(copy_path='/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/')\n# img = Image.open('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/00004.jpg')\n# width, height = img.size\n# bb_x1 = int(cars_train_annotations[cars_train_annotations['img_name'] == '00004.jpg']['bb_x1'])\n# bb_x2 = int(cars_train_annotations[cars_train_annotations['img_name'] == '00004.jpg']['bb_x2'])\n# bb_y1 = int(cars_train_annotations[cars_train_annotations['img_name'] == '00004.jpg']['bb_y1'])\n# bb_y2 = int(cars_train_annotations[cars_train_annotations['img_name'] == '00004.jpg']['bb_y2'])\n# img2 = img.crop((bb_x1, bb_x2, bb_y1, bb_y2))\n\n# #print(width, height, bb_x1, bb_x2, bb_y1, bb_y2)\n# img2.save(copy_path + filename)\n# #img2 = Image.open('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy 2/a_00004.jpg')\n# img2.show()"},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":"# # Convert .mat to Python dict with Scipy.io\n\n# sc_devkit = '/Users/katerina/Workspace/visual_census/data/devkit'\n# cars_meta = scipy.io.loadmat(os.path.join(sc_devkit, 'cars_meta.mat'))['class_names']\n# cars_train_annos = scipy.io.loadmat(os.path.join(sc_devkit, 'cars_train_annos.mat'))['annotations']"},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":"# # Extract Car Metadata from dictionary to an array\n\n# car_makes = []\n# for idx, vehicle in enumerate(cars_meta[0]):\n#     car_makes.append([vehicle[0],vehicle[0].split(' ')[0], vehicle[0].split(' ')[-1]])\n# car_makes = pd.DataFrame(car_makes, columns=['full_label', 'mnfr', 'year'])\n# car_make_nums = np.array(car_makes.index)"},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":"# # Extract Car Labels from dictionary to a list\n# # List used because both int (label) and str (img id) required\n\n# cars_train_labels = {}\n# cars_train_annotations = []\n# for idx, anno in enumerate(cars_train_annos[0]):\n#     cars_train_labels[anno[5][0]] = anno[4][0][0]\n#     cars_train_annotations.append([anno[0][0][0], anno[1][0][0], anno[2][0][0], anno[3][0][0], anno[4][0][0], anno[5][0]])\n# cars_train_annotations = pd.DataFrame(cars_train_annotations, columns=['bb0', 'bb1', 'bb2', 'bb3', 'label', 'img_name'])"},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":"# sc_cars_train = '/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy'\n# sc_cars_test = '/Users/katerina/Workspace/visual_census/data/training_data/cars_test'\n# train_data_dir = pathlib.Path(sc_cars_train)\n# test_data_dir = pathlib.Path(sc_cars_test)"},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":"# train_image_count = len(list(train_data_dir.glob('*.jpg')))\n# test_image_count = len(list(test_data_dir.glob('*.jpg')))"},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":"# train_image_names = np.array([item.name for item in train_data_dir.glob('*')])\n# test_image_names = np.array([item.name for item in test_data_dir.glob('*')])"},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":"# list_ds = tf.data.Dataset.list_files(str('/Users/katerina/Workspace/visual_census/data/training_data/cars_train copy/*'))\n# list_ds_test = tf.data.Dataset.list_files(str('/Users/katerina/Workspace/visual_census/data/training_data/cars_test/*'))"},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":"# BATCH_SIZE = 32\n# IMG_HEIGHT = 224\n# IMG_WIDTH = 224\n# STEPS_PER_EPOCH = np.ceil(train_image_count/BATCH_SIZE)"},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":"# def get_label(file_path):\n#   # convert the path to a list of path components\n#   parts = tf.strings.split(tf.strings.split(file_path, '_')[-1], '.')[0]\n#   # The last is the image name\n#   return parts==np.array([str(idx) for idx in car_makes.index])"},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":"# def decode_img(img):\n#   # convert the compressed string to a 3D uint8 tensor\n#   img = tf.image.decode_jpeg(img, channels=3)\n#   # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n#   img = tf.image.convert_image_dtype(img, tf.float32)\n#   # resize the image to the desired size.\n#   return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])"},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":"# def process_path(file_path):\n#   label = get_label(file_path)\n#   # load the raw data from the file as a string\n#   img = tf.io.read_file(file_path)\n#   img = decode_img(img)\n#   return img, label"},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":"# AUTOTUNE = tf.data.experimental.AUTOTUNE\n# labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n# labeled_ds_test = list_ds_test.map(process_path, num_parallel_calls=AUTOTUNE)"},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":"# def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n  \n#   if cache:\n#     if isinstance(cache, str):\n#       ds = ds.cache(cache)\n#     else:\n#       ds = ds.cache()\n\n#   ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n\n#   # Repeat forever\n#   ds = ds.repeat()\n\n#   ds = ds.batch(BATCH_SIZE)\n\n#   # `prefetch` lets the dataset fetch batches in the background while the model\n#   # is training.\n#   ds = ds.prefetch(buffer_size=AUTOTUNE)\n\n#   return ds"},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":"# train_ds = prepare_for_training(labeled_ds)\n# image_batch, label_batch = next(iter(train_ds))"},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":"# test_ds = labeled_ds_test.prefetch(buffer_size=AUTOTUNE)\n# image_batch_test, label_batch_test = next(iter(test_ds))"},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":"#np.array([x[label][0] for label in label_batch]).reshape(-1,1)"},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":"# def show_batch(image_batch, label_batch):\n#   plt.figure(figsize=(10,10))\n#   for n in range(25):\n#       ax = plt.subplot(5,5,n+1)\n#       plt.imshow(image_batch[n])\n#       plt.title(car_make_nums[label_batch[n]==1][0])\n#       plt.axis('off')\n\n# show_batch(image_batch.numpy(), label_batch.numpy())"},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":"# plt.figure(figsize=(10,10))\n# for i in range(25):\n#     plt.subplot(5,5,i+1)\n#     plt.xticks([])\n#     plt.yticks([])\n#     plt.grid(False)\n#     plt.imshow(image_batch[i], cmap=plt.cm.binary)\n#     plt.xlabel(np.array(car_makes['mnfr'])[label_batch[i]])\n# plt.show()"},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","\n","model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(196, activation='softmax'))"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 222, 222, 32)      896       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 111, 111, 32)      0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 109, 109, 64)      18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 54, 54, 64)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 52, 52, 64)        36928     \n_________________________________________________________________\nflatten (Flatten)            (None, 173056)            0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                11075648  \n_________________________________________________________________\ndense_1 (Dense)              (None, 196)               12740     \n=================================================================\nTotal params: 11,144,708\nTrainable params: 11,144,708\nNon-trainable params: 0\n_________________________________________________________________\n"}],"source":["model.summary()"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":"cars_numerical = np.array(car_makes.index)\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit((images_decoded, labels), epochs=10)"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}